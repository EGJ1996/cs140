			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Naftali Harris <naftali@stanford.edu>
Luke Pappas <lpappas9@stanford.edu>
Connor Woodson <cwoodson@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* One sleeping thread's semaphore lock, as part of a list */
struct sleeping_thread {
  struct list_elem elem; /* List Element */
  struct semaphore semaphore; /* Semaphore */
  int64_t wake_time; /* Time to wake this thread */
};

Purpose: Store semaphores associated with sleeping threads in a list.

/*list of sleeping threads */
static struct list sleeping_threads_list;

Purpose: Store the list of sleeping_thread structs, ordered by wake time.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

The function starts with ensuring a correct environment
(e.g. interrupts are on, sleep_ticks > 0, ...).

Then we create an instance of our sleeping_thread struct and
initialize the semaphore with a value of 0, and set wake_time.
The reason we set the semaphore's value to 0 is that it allows us
to automatically block on it, and then wake when it is signaled.
The wake_time is simply the current number of timer_ticks + the
sleep time, which is expressed in terms of ticks and passed in as a 
param to this function. 

Then we disable interrupts to add our struct to the list of
sleeping thread structs. The reason we disable interrupts is to 
ensure that no race conditions happen between insertion into the 
sleeping_threads_list and removal from the sleeping_threads_list. 
Because the timer interrupt handler consults this list, our only
option is to disable interrupts, as interrupt handlers cannot block
and thus cannot acquire locks/semaphores...

Finally, with interrupts re-enabled,
we call sema_down. Because we initialize the semaphore to 0, this
causes the thread to block, until the timer interrupt handler calls
sema_up, and effectively wakes the thread from its sleep. 

The timer interrupt handler has an important role in this process. Threads
go to sleep in the timer_sleep function by calling sema_down on the semaphore
they themselves set to 0. The timer interrupt handler is responsible for
waking these threads up by calling sema_up on the semaphores contained
in each sleeping_thread for which wake time has been reached. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

As we have learned in class and from the documentation of the assignment, 
one of the goals of system programming is to minimize the amount of time
spent in an interrupt handler, because that will cause a thread to burn cpu
time. Thus, in the timer interrupt handler, we are conscience of this timing
issue. With respect to the process of timer_sleep, the timer interrupt handler 
is responsible for waking up the threads who's wake times have been reached. To 
reduce the amount of time spent in the timer interrupt handler with this task, 
we do the following:
- When adding a new sleeping_thread to the sleeping_threads_list, we use the
insert comparison function sleeping_thread_insert_func, to ensure that threads
are inserted in order according to their wake_time. The earliest wake times are
placed at the front of the list, and the later the times, the farther back in the 
list the thread will be. 
- When the timer interrupt handler reaches the code to wake up the sleeping
threads, it looks at the front of the list only. If the wake time of the thread
at the front of the list has been reached, the timer interrupt handler will
singal that thread to wake up by calling sema_up on the semaphore in the sleeping_thread. 
The timer interrupt hanlder than repeats this process, until it reaches a thread who's wake time has not been reached, or gets to the end of the list. In the case of the
first wake time that has not been reached, because the sleeping_thread_list is ordered
by wake time, we know all sleeping_threads that follow have equal to or later
wake times, and thus we can end our traversal of this list and move on to 
the next responsibility in the time interrupt handler. 

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

We inserted nice and recent_cpu fields into each thread struct to keep track
of these values for each thread.

struct thread
  {
    ...
    int32_t nice;                       /* Nice value for BSD scheduler */
    fp_float recent_cpu;                /* Recent cpu used for BSD scheduler */
    ...
  };

We keep track of the global load average:

static fp_float load_avg;

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     B
12      8   4   0  61  60  59     A
16      12  4   0  60  60  59     B
20      12  8   0  60  59  59     A
24      16  8   0  59  59  59     C
28      16  8   4  59  59  58     B
32      16  12  4  59  58  58     A
36      20  12  4  58  58  58     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes: When a thread's priority changes, the spec was unclear as to where in the
new queue it ought to be placed. Our implementation used just one ready queue,
and after thread_yield it would place the yielding thread in the back of the
queue. The resulting behavior is equivalent to having 64 queues and placing a
thread with a new priority in the back of the queue. This is the same rule we
applied to the above table.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

The code that updates the recent_cpu and priority of the threads occurs in the
timer interrupt handler. It has to, because it must happen every fixed number of
ticks. The scheduling logic, however, where we select which thread to run,
happens outside of an interrupt context.

Every four ticks, we need to adjust the priority for the threads that had their
recent_cpu adjusted. This has negligible cost, since this could have been
adjusted for at most four threads, and so the fact that it occurs in an
interrupt context has no substantial performance cost.

Every second, we must also adjust the recent_cpu and priority of all the
threads.  This also occurs in the interrupt context, but since it happens only
once a second, it does not introduce substantial performance loss. We also take
care to compute relatively efficiently when updating the recent_cpu and
priority in the timer interrupt; in particular we compute the 2 * load_factor /
(2 * load_factor + 1) value and use it for all threads.

Since we only use one queue, the scheduler runs in time proportional to the
number of processes in the READY state. If we had many such processes, this
could potentially take a while, leading to performance degradation.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Having one ready queue instead of 64 is less efficient where there are a large
number of processes. However, if only the bottom ready queues had threads in
them, perhaps because all of ready threads had large nice values, then you would
need to loop through the top ready queue until you found the first non-empty
queue.

Because of this, if we had extra time we would have implemented the 64
ready queues, but also included a global max_priority variable, which would
indicate the largest thready priority, to avoid the above problem with empty
high-priority queues.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

Yes! Some of our tests failed with jitter. We debugged them for hours until we
figured out that the bugs were actually in the tests, not our code.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
