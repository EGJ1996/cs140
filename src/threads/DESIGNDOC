			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Naftali Harris <naftali@stanford.edu>
Luke Pappas <lpappas9@stanford.edu>
Connor Woodson <cwoodson@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* One sleeping thread's semaphore lock, as part of a list */
struct sleeping_thread {
  struct list_elem elem; /* List Element */
  struct semaphore semaphore; /* Semaphore */
  int64_t wake_time; /* Time to wake this thread */
};

Purpose: Store semaphores associated with sleeping threads in a list.

/*list of sleeping threads */
static struct list sleeping_threads_list;

Purpose: Store the list of sleeping_thread structs, ordered by wake time.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

The function starts with ensuring a correct environment
(e.g. interrupts are on, sleep_ticks > 0, ...).

Then we create an instance of our sleeping_thread struct and initialize the
semaphore with a value of 0, and set wake_time.  The reason we set the
semaphore's value to 0 is that it allows us to automatically block on it, and
then wake when it is signaled.  The wake_time is simply the current number of
timer_ticks + the sleep time, which is expressed in terms of ticks and passed
in as a param to this function. 

Then we disable interrupts to add our struct to the list of sleeping thread
structs. The reason we disable interrupts is to ensure that no race conditions
happen between insertion into the sleeping_threads_list and removal from the
sleeping_threads_list.  Because the timer interrupt handler consults this list,
our only option is to disable interrupts, as interrupt handlers cannot block
and thus cannot acquire locks/semaphores...

Finally, with interrupts re-enabled, we call sema_down. Because we initialize
the semaphore to 0, this causes the thread to block, until the timer interrupt
handler calls sema_up, and effectively wakes the thread from its sleep. 

The timer interrupt handler has an important role in this process. Threads go
to sleep in the timer_sleep function by calling sema_down on the semaphore they
themselves set to 0. The timer interrupt handler is responsible for waking
these threads up by calling sema_up on the semaphores contained in each
sleeping_thread for which wake time has been reached. 

>> A3: What steps are taken to minimize the amount of time spent in the timer
>> interrupt handler?

As we have learned in class and from the documentation of the assignment, one
of the goals of system programming is to minimize the amount of time spent in
an interrupt handler, because that will cause a thread to burn cpu time. Thus,
in the timer interrupt handler, we are conscience of this timing issue. With
respect to the process of timer_sleep, the timer interrupt handler is
responsible for waking up the threads who's wake times have been reached. To
reduce the amount of time spent in the timer interrupt handler with this task,
we do the following:

- When adding a new sleeping_thread to the sleeping_threads_list, we use the
  insert comparison function sleeping_thread_insert_func, to ensure that
threads are inserted in order according to their wake_time. The earliest wake
times are placed at the front of the list, and the later the times, the farther
back in the list the thread will be. 

- When the timer interrupt handler reaches the code to wake up the sleeping
threads, it looks at the front of the list only. If the wake time of the
thread at the front of the list has been reached, the timer interrupt handler
will signal that thread to wake up by calling sema_up on the semaphore in the
sleeping_thread.  The timer interrupt handler than repeats this process, until
it reaches a thread who's wake time has not been reached, or gets to the end of
the list. In the case of the first wake time that has not been reached, because
the sleeping_thread_list is ordered by wake time, we know all sleeping_threads
that follow have equal to or later wake times, and thus we can end our
traversal of this list and move on to the next responsibility in the time
interrupt handler. 

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Within our implementation of timer_sleep, the only critical region of the 
code that is subject to race conditions is the call to list_insert_ordered, 
adding our new sleeping thread to the global static list. 
To avoid race conditions, we disable interrupts around this function call.
Therefore, at most one thread at a time can insert into the list of sleeping
threads from timer_sleep. If multiple threads were to call the function, each would
set up its own sleeping thread on its stack and get its own timer tick value, and
then one at a time would insert into the list of sleeping_threads due to the 
interrupts being disabled. 

The reason that we disable interrupts is because it is our only option 
in this case. Given that the sleeping_threads_list is accessed in the timer
interrupt handler, we cannot use locks, semaphoresâ€¦ because the interrupt
handler cannot block or sleep. Therefore, our only option to avoid synchronization
issues is to disable interrupts when inserting a sleeping_thread struct into
this list, thus ensuring that the thread who disabled the interrupts will not
get preempted while it is inserting into the list. 

Immediately after we insert the sleeping_thread struct into the list, we
restore interrupts to their old level, thus reducing the amount of 
code that operates with interrupts disabled to a bare minimum. 

Finally, we note that there are no race conditions any where else in the 
timer_sleep function, as the sleeping_thread struct is created on the 
stack, and is thus internal to the executing thread, and timer_ticks(), 
which we use to get the time at which the sleep was called, is itself 
locked down with interrupts disabled (given in the source code), therefore
eliminating race conditions or concurrency issues there as well. 

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

If a timer interrupt were to occur during a call to timer sleep, there are 
several potential issues. 

The first is that the value of timer ticks could be skewed, as it is read in
timer_sleep, and incremented in the timer interrupt handler. However, the source
addresses this concurrency issue by disabling interrupts in the timer ticks function.
After we get the initial timer_ticks() value as the "time" at which the timer_sleep 
function was invoked, if a timer interrupt were to fire after this, it would not be a 
problem, as the logic of computing wake_time is only dependent on this initial "time"
at which timer_sleep is called. 

As described above, if the timer interrupt were to fire mid execution of the 
timer_sleep function, than it could cause preemption to occur. As stated above, 
the only issue with preemption involves inserting a sleeping_thread struct into
the list of sleeping_threads_list. However, because we disable interrupts, we
neutralize this issue. 

Furthermore, if a timer_interrupt were to fire mid execution of the timer_sleep()
function, than the timer interrupt handler would run. Without any concurrency 
directives in place, this would present an issue, as the timer interrupt handler
accesses the list of sleeping_threads to wake the ones who's time has come. 
However, because we disable interrupts when accessing the list of 
sleeping_threads in the timer_sleep function, and because interrupts are obviously 
disabled in interrupt handlers, the issue is neutralized. 

Thus at all times, at most one person can be accessing the list of 
sleeping_threads. 

Finally, we also address the case where the timer interrupt is fired mid execution 
of timer_sleep, before the sleeping_thread struct is added to the list
of sleeping_threads. If an interrupt fires before this point, it is possible that
the currently executing thread get preempted, and not run again until its wake time 
has been met, or that the single timer interrupt was the only sleep time 
requested. In such a case, we check if the current value of timer ticks equals 
the wake time. If it indeed does, or if the current time is greater than the wake time
we can return from the function and avoid the unnecessary adding of the sleeping_thread
to the list, only to have it signaled and removed on the following timer 
interrupt. 




---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread
{
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    struct list_elem allelem;           /* List element for all threads list. */
    
    /* List of locks this thread currently holds */
    /* Used for priority donationa */
    struct list locks_held;
    
    /* Dummy lock containing this threads original priority */
    struct lock original_priority_info;
    
    /* The lock this thread is waiting on. Null if not waiting */
    struct lock* lock_waiting_on;
    
    /* threads nice value. For bsd_scheduler */
    int nice;
    
    /* Thread's recent cpu. For the bsd_scheduler */
    fp_float recent_cpu;
    
    /* Allows this thread to be placed in a list of threads who's */
    /* recent cpu value has changed */
    struct list_elem cpu_list_elem;
    
    /* True if the threads recent_cpu has changed and the thread */
    /* has not been updated yet */
    bool cpu_has_changed;
    
    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */
    
#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif
    
    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
};

Purpose: We added a list of locks, a lock, and a pointer to a lock to the thread struct.
locks_held = allows us to track the highest priority donated for each lock
		currently held by the thread.
original_priority_info = dummy lock inserted into locks_held, containing the threads
		original priority info. Allows for easy priority shedding.
lock_waiting_on = pointer to the lock this thread is currently waiting on, null if non. 
		Allows for priority donation to propagate in the nested case.

/* Lock. */
struct lock
{
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    
    int priority;             /* LP for donation purposes */
    struct list_elem elem;     /* LP to allow locks to be placed in lists */
};

Purpose: we added an int priority and a list_elem.
priority: allows us to track the highest priority donated for this lock.
list_elem: allows the lock to be inserted into a list.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

In designing our system of priority donation, we had two goals in mind. The 
first was to ensure that we addressed all the cases of priority donation specified
in the lab section and in the handout, namely multiple donation, standard donation, 
and nested donation. 

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

We inserted nice and recent_cpu fields into each thread struct to keep track
of these values for each thread.

struct thread
  {
    ...
    int32_t nice;                       /* Nice value for BSD scheduler */
    fp_float recent_cpu;                /* Recent cpu used for BSD scheduler */
    ...
  };

We keep track of the global load average:

static fp_float load_avg;

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     B
12      8   4   0  61  60  59     A
16      12  4   0  60  60  59     B
20      12  8   0  60  59  59     A
24      16  8   0  59  59  59     C
28      16  8   4  59  59  58     B
32      16  12  4  59  58  58     A
36      20  12  4  58  58  58     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes: When a thread's priority changes, the spec was unclear as to where in the
new queue it ought to be placed. Our implementation used just one ready queue,
and after thread_yield it would place the yielding thread in the back of the
queue. The resulting behavior is equivalent to having 64 queues and placing a
thread with a new priority in the back of the queue. This is the same rule we
applied to the above table.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

The code that updates the recent_cpu and priority of the threads occurs in the
timer interrupt handler. It has to, because it must happen every fixed number of
ticks. The scheduling logic, however, where we select which thread to run,
happens outside of an interrupt context.

Every four ticks, we need to adjust the priority for the threads that had their
recent_cpu adjusted. This has negligible cost, since this could have been
adjusted for at most four threads, and so the fact that it occurs in an
interrupt context has no substantial performance cost.

Every second, we must also adjust the recent_cpu and priority of all the
threads.  This also occurs in the interrupt context, but since it happens only
once a second, it does not introduce substantial performance loss. We also take
care to compute relatively efficiently when updating the recent_cpu and
priority in the timer interrupt; in particular we compute the 2 * load_factor /
(2 * load_factor + 1) value and use it for all threads.

Since we only use one queue, the scheduler runs in time proportional to the
number of processes in the READY state. If we had many such processes, this
could potentially take a while, leading to performance degradation.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Having one ready queue instead of 64 is less efficient where there are a large
number of processes. However, if only the bottom ready queues had threads in
them, perhaps because all of ready threads had large nice values, then you would
need to loop through the top ready queue until you found the first non-empty
queue.

Because of this, if we had extra time we would have implemented the 64
ready queues, but also included a global max_priority variable, which would
indicate the largest thready priority, to avoid the above problem with empty
high-priority queues.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

Yes! Some of our tests failed with jitter. We debugged them for hours until we
figured out that the bugs were actually in the tests, not our code.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

We felt that the introductory session should have been more an introduction to
the homework assignment, rather than an overview of threads.

>> Any other comments?
